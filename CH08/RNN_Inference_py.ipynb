{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install afs2-model\n",
    "!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from afs import models\n",
    "import os, numpy, pandas, datetime, time, json, requests, re, configparser\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from flask import Flask, jsonify\n",
    "from flask import request\n",
    "import json\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX = []\n",
    "    for i in range(len(dataset)-look_back+1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "    return numpy.array(dataX)\n",
    "\n",
    "def load_rnn_model(filepath, dataset, count=1):\n",
    "    print('load rnn')\n",
    "    model = load_model(filepath)\n",
    "    raw_data = dataset\n",
    "    print('raw_data: {}'.format(raw_data))\n",
    "    testPredict = model.predict(dataset)\n",
    "    return raw_data, testPredict\n",
    "\n",
    "afs_models = models()\n",
    "app = Flask(__name__)\n",
    "# downland model\n",
    "@app.route(\"/model\", methods=['POST'])\n",
    "def model():\n",
    "    data=request.get_json()\n",
    "    save_path = data['save_model_to']\n",
    "    model_repository_name = data['name']\n",
    "    model_name = data['version']\n",
    "    save_path = os.path.join(save_path,model_repository_name)\n",
    "    afs_models.download_model(save_path=save_path, model_repository_name=model_repository_name, model_name=model_name)\n",
    "    return jsonify({'status': 'success','model':save_path})\n",
    "\n",
    "@app.route(\"/predict\", methods=['POST'])\n",
    "def predict():\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    data_list3 = []\n",
    "    data_list_all = []\n",
    "    post_data1 = request.form.get('feature_1')\n",
    "    post_data2 = request.form.get('feature_2')\n",
    "    for value in post_data1.split(','):\n",
    "        data_list1.append(float(value))\n",
    "    for value in post_data2.split(','):\n",
    "        data_list2.append(float(value))\n",
    "    data_list3.append(data_list1)\n",
    "    data_list3.append(data_list2)\n",
    "    data_list_all.append(data_list3)\n",
    "    data = numpy.array(data_list_all)\n",
    "    raw_data, predict_data = load_rnn_model('/rnn_model.h5', data)\n",
    "    for datas in predict_data:\n",
    "        for data in datas:\n",
    "            predict_data = data\n",
    "    return jsonify({'message': str(predict_data)})\n",
    "\n",
    "@app.route(\"/check_model\", methods=['POST'])\n",
    "def check_model():\n",
    "    data = request.get_json()\n",
    "    filepath = data['filepath']\n",
    "    if os.path.isfile(filepath):\n",
    "        print(\"EXISTS\")\n",
    "        return jsonify({'status': 'EXISTS','model':filepath})\n",
    "    else:\n",
    "        print(\"NOT EXISTS\")\n",
    "        return jsonify({'status': 'NOT EXISTS','model':filepath})\n",
    "    \n",
    "import os\n",
    "if __name__ == \"__main__\":\n",
    "    port = int(os.environ.get('cloud_inference_port', 5000))\n",
    "     #port = int('7500')\n",
    "    app.run(host='0.0.0.0', port=port)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
